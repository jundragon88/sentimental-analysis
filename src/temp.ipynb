{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "    Sentiment Analysis \n",
    "    (Exercise script. Some functions are not implemented. )\n",
    "\n",
    "        Author : Sangkeun Jung (2017)\n",
    "        - using Tensorflow\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, inspect\n",
    "\n",
    "# add common to path\n",
    "from pathlib import Path\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "common_path = str(Path(currentdir).parent.parent)\n",
    "sys.path.append( common_path )\n",
    "\n",
    "from common.nlp.vocab import Vocab\n",
    "from common.nlp.data_loader import N21TextData\n",
    "from common.nlp.converter import N21Converter\n",
    "\n",
    "from dataset import SentimentDataset\n",
    "from dataset import load_data\n",
    "from common.ml.hparams import HParams\n",
    "\n",
    "import numpy as np\n",
    "import copy \n",
    "import time \n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn\n",
    "from tensorflow.contrib.layers.python.layers import linear\n",
    "\n",
    "from common.ml.tf.deploy import freeze_graph\n",
    "\n",
    "print( \"Tensorflow Version : \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysis():\n",
    "    def __init__(self, hps, mode=\"train\"):\n",
    "        self.hps = hps\n",
    "        self.x = tf.placeholder(tf.int32,   [None, hps.num_steps], name=\"pl_tokens\")\n",
    "        self.y = tf.placeholder(tf.int32,   [None], name=\"pl_target\")\n",
    "        self.w = tf.placeholder(tf.float32, [None, hps.num_steps], name=\"pl_weight\")\n",
    "        self.keep_prob = tf.placeholder(tf.float32, [], name=\"pl_keep_prob\")\n",
    "\n",
    "        ### 4 blocks ###\n",
    "        # 1) embedding\n",
    "        # 2) dropout on input embedding\n",
    "        # 3) sentence encoding using rnn\n",
    "        # 4) encoding to output classes\n",
    "        # 5) loss calcaulation\n",
    "\n",
    "        def _embedding(x):\n",
    "            # character embedding \n",
    "            print(\"-\"*100)\n",
    "            print(\"Implement function '{}'\".format(_embedding.__name__))\n",
    "            print(\"Keywords\")\n",
    "            print(\"\\t - tf.initializers.variance_scaling\")\n",
    "            print(\"\\t - tf.get_variable\")\n",
    "            print(\"\\t - tf.nn.embedding_lookup\")\n",
    "            print(\"\\t - tf.unstack\")\n",
    "\n",
    "            print('Input : Tensor(\"model/pl_tokens:0\", shape=(?, 128), dtype=int32)')\n",
    "\n",
    "            \n",
    "            print(\"Return: a list of <tf.Tensor shape=(?, 50) dtype=float32>\")\n",
    "            print(\"        len(a_list) = 128\")\n",
    "\n",
    "            step_inputs = None # should be implemented\n",
    "            return step_inputs\n",
    "\n",
    "        def _sequence_dropout(step_inputs, keep_prob):\n",
    "            # apply dropout to each input\n",
    "            # input : a list of input tensor which shape is [None, input_dim]\n",
    "            print(\"-\"*100)\n",
    "            with tf.name_scope('sequence_dropout') as scope:\n",
    "                print(\"Implement step_outputs\")\n",
    "                print(\"Keywords\")\n",
    "                print(\"\\t - tf.nn.dropout\")\n",
    "\n",
    "\n",
    "\n",
    "            print(\"Return: a list of <tf.Tensor shape=(?, 50) dtype=float32>\")\n",
    "            step_outputs = None # should be implemented\n",
    "            return step_outputs\n",
    "\n",
    "        def sequence_encoding_n21_rnn(step_inputs, cell_size, scope_name):\n",
    "            # rnn based N21 encoding (GRU)\n",
    "            print(\"-\"*100)\n",
    "            print(\"Implement function '{}'\".format(sequence_encoding_n21_rnn.__name__))\n",
    "            print('Input : a list of <tf.Tensor shape=(?, 50), dtype=float32>')\n",
    "            print(\"Keywords\")\n",
    "            print(\"\\t - tf.contrib.rnn.GRUCell\")\n",
    "            print(\"\\t - tf.contrib.rnn.static_rnn\")\n",
    "\n",
    "            print(\"Return: a list of <tf.Tensor, shape=(?, 100)>\")\n",
    "            out = None # should be implemented\n",
    "            return out\n",
    "\n",
    "        def _to_class(input, num_class):\n",
    "            print(\"-\"*100)\n",
    "            print(\"Implement function '{}'\".format(_to_class.__name__))\n",
    "            print('Input : tf.Tensor, shape=(?, 100), dtype=float32')\n",
    "            print(\"Keywords\")\n",
    "            print(\"\\t - tensorflow.contrib.layers.python.layers.linear\")\n",
    "\n",
    "            print(\"Return: tf.Tensor, shape=(?, 4), dtype=float32\")\n",
    "            out = None # should be implemented\n",
    "            return out\n",
    "\n",
    "        def _loss(out, ref):\n",
    "            # out : [batch_size, num_class] float - unscaled logits\n",
    "            # ref : [batch_size] integer\n",
    "            # calculate loss function using cross-entropy\n",
    "            print(\"-\"*100)\n",
    "            print('Input out: tf.Tensor, shape=(?, 4)')\n",
    "            print('Input ref: tf.Tensor(\"model/pl_target:0\", shape=(?,), dtype=int32)')\n",
    "\n",
    "            print(\"Keywords\")\n",
    "            print(\"\\t - tf.nn.sparse_softmax_cross_entropy_with_logits\")\n",
    "            print(\"\\t - tf.reduce_mean\")\n",
    "            \n",
    "            print(\"Return: tf.Tensor, shape=(), dtype=float32\")\n",
    "            loss = None # should be implemented\n",
    "            return loss\n",
    "        \n",
    "        seq_length    = tf.reduce_sum(self.w, 1) # [batch_size]\n",
    "\n",
    "        step_inputs   = _embedding(self.x)\n",
    "        step_inputs   = _sequence_dropout(step_inputs, self.keep_prob)\n",
    "        sent_encoding = sequence_encoding_n21_rnn(step_inputs, hps.enc_dim, scope_name=\"encoder\")\n",
    "        out           = _to_class(sent_encoding, hps.num_target_class)\n",
    "        loss          = _loss(out, self.y) \n",
    "\n",
    "        if loss is None: \n",
    "            print(\"All functions should be implemented!\")\n",
    "            import sys; sys.exit()\n",
    "\n",
    "\n",
    "        out_probs     = tf.nn.softmax(out, name=\"out_probs\")\n",
    "        out_pred      = tf.argmax(out_probs, 1, name=\"out_pred\")\n",
    "\n",
    "        self.loss      = loss\n",
    "        self.out_probs = out_probs\n",
    "        self.out_pred  = out_pred\n",
    "\n",
    "        self.global_step = tf.get_variable(\"global_step\", [], tf.int32, initializer=tf.zeros_initializer, trainable=False)\n",
    "\n",
    "        if mode == \"train\":\n",
    "            optimizer       = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "            self.train_op   = optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "        else:\n",
    "            self.train_op = tf.no_op()\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_hparams():\n",
    "        return HParams(\n",
    "            learning_rate     = 0.001,\n",
    "            keep_prob         = 0.5,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_id_data, num_vocabs, num_taget_class):\n",
    "    #\n",
    "    # train sentiment analysis using given train_id_data\n",
    "    #\n",
    "    max_epoch = 100\n",
    "    model_dir = \"./trained_models\"\n",
    "    hps = SentimentAnalysis.get_default_hparams()\n",
    "    hps.update(\n",
    "                    batch_size= 100,\n",
    "                    num_steps = 128,\n",
    "                    emb_size  = 50,\n",
    "                    enc_dim   = 100,\n",
    "                    vocab_size=num_vocabs,\n",
    "                    num_target_class=num_taget_class\n",
    "               )\n",
    "\n",
    "    with tf.variable_scope(\"model\"):\n",
    "        model = SentimentAnalysis(hps, \"train\")\n",
    "\n",
    "    sv = tf.train.Supervisor(is_chief=True,\n",
    "                             logdir=model_dir,\n",
    "                             summary_op=None,  \n",
    "                             global_step=model.global_step)\n",
    "\n",
    "    # tf assign compatible operators for gpu and cpu \n",
    "    tf_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "    with sv.managed_session(config=tf_config) as sess:\n",
    "        local_step       = 0\n",
    "        prev_global_step = sess.run(model.global_step)\n",
    "\n",
    "        train_data_set = SentimentDataset(train_id_data, hps.batch_size, hps.num_steps)\n",
    "        losses = []\n",
    "        while not sv.should_stop():\n",
    "            fetches = [model.global_step, model.loss, model.train_op]\n",
    "            a_batch_data = next( train_data_set.iterator )\n",
    "            y, x, w = a_batch_data\n",
    "            fetched = sess.run(fetches, {\n",
    "                                            model.x: x, \n",
    "                                            model.y: y, \n",
    "                                            model.w: w,\n",
    "\n",
    "                                            model.keep_prob: hps.keep_prob,\n",
    "                                        }\n",
    "                              )\n",
    "\n",
    "            local_step += 1\n",
    "\n",
    "            _global_step = fetched[0]\n",
    "            _loss        = fetched[1]\n",
    "            losses.append( _loss )\n",
    "            if local_step < 10 or local_step % 10 == 0:\n",
    "                epoch = train_data_set.get_epoch_num()\n",
    "                print(\"Epoch = {:3d} Step = {:7d} loss = {:5.3f}\".format(epoch, _global_step, np.mean(losses)) )\n",
    "                _loss = []                \n",
    "                if epoch >= max_epoch : break \n",
    "\n",
    "        print(\"Training is done.\")\n",
    "    sv.stop()\n",
    "\n",
    "    # model.out_pred, model.out_probs\n",
    "    freeze_graph(model_dir, \"model/out_pred,model/out_probs\", \"frozen_graph.tf.pb\") ## freeze graph with params to probobuf format\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.core.framework import graph_pb2\n",
    "def predict(token_vocab, target_vocab, sent):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force to use cpu only (prediction)\n",
    "    model_dir = \"./trained_models\"\n",
    "\n",
    "    # prepare sentence converting\n",
    "    # to make raw sentence to id data easily\n",
    "    in_sent = '{}\\t{}'.format('___DUMMY_CLASS___', sent)\n",
    "    pred_data     = N21TextData(in_sent, mode='sentence')\n",
    "    pred_id_data  = N21Converter.convert(pred_data, target_vocab, token_vocab)\n",
    "    pred_data_set = SentimentDataset(pred_id_data, 1, 128)\n",
    "\n",
    "    #\n",
    "    a_batch_data = next(pred_data_set.predict_iterator) # a result\n",
    "    b_sentiment_id, b_token_ids, b_weight = a_batch_data\n",
    "\n",
    "    # Restore graph\n",
    "    # note that frozen_graph.tf.pb contains graph definition with parameter values in binary format\n",
    "    _graph_fn =  os.path.join(model_dir, 'frozen_graph.tf.pb')\n",
    "    with tf.gfile.GFile(_graph_fn, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    with tf.Session(graph=graph) as sess:\n",
    "        # to check load graph\n",
    "        #for n in tf.get_default_graph().as_graph_def().node: print(n.name)\n",
    "\n",
    "        # make interface for input\n",
    "        pl_token     = graph.get_tensor_by_name('import/model/pl_tokens:0')\n",
    "        pl_keep_prob = graph.get_tensor_by_name('import/model/pl_keep_prob:0')\n",
    "\n",
    "        # make interface for output\n",
    "        out_pred  = graph.get_tensor_by_name('import/model/out_pred:0')\n",
    "        out_probs = graph.get_tensor_by_name('import/model/out_probs:0')\n",
    "        \n",
    "\n",
    "        # predict sentence \n",
    "        b_best_pred_index, b_pred_probs = sess.run([out_pred, out_probs], feed_dict={\n",
    "                                                                                        pl_token : b_token_ids,\n",
    "                                                                                        pl_keep_prob : 1.0,\n",
    "                                                                                    }\n",
    "                                          )\n",
    "\n",
    "        best_pred_index = b_best_pred_index[0]\n",
    "        pred_probs = b_pred_probs[0]\n",
    "\n",
    "        best_target_class = target_vocab.get_symbol(best_pred_index)\n",
    "        print( pred_probs[best_pred_index] )\n",
    "        best_prob  = int( pred_probs[best_pred_index] )\n",
    "        print(best_target_class, best_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id_data, token_vocab, target_vocab = load_data()\n",
    "    num_vocabs       = token_vocab.get_num_tokens()\n",
    "    num_target_class = target_vocab.get_num_targets()\n",
    "\n",
    "\n",
    "    train(train_id_data, num_vocabs, num_target_class)\n",
    "    predict(token_vocab, target_vocab, '유일한 단점은 방안에 있어도 들리는 소음인데요.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
