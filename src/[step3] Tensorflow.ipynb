{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM을 이용한 영화 리뷰 감정 분석\n",
    "Word2Vec을 통해 임베딩 된 네이버 영화 리뷰 데이터를 BiLSTM을 통해 긍정, 부정을 분류해주는 예제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\dev-tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Bi_LSTM\n",
    "import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data\"\n",
    "TRAIN_DATA = DATA_PATH + \"/ratings_train.txt\"\n",
    "TEST_DATA = DATA_PATH + \"/ratings_test.txt\"\n",
    "MODEL_DATA = DATA_PATH + \"/Word2Vec.model\"\n",
    "META_DATA = DATA_PATH + \"/metadata.tsv\"\n",
    "WORD2VEC_PATH = DATA_PATH + \"/word2vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V = Word2Vec.Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = W2V.read_data(TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize Start!\n",
      "Could take minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\dev-tensorflow\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize Done!\n"
     ]
    }
   ],
   "source": [
    "## tokenize the data we have\n",
    "print(\"Tokenize Start!\\nCould take minutes...\")\n",
    "tokens = [[W2V.tokenize(row[1]),int(row[2])] for row in train_data if W2V.tokenize(row[1]) != []]\n",
    "tokens = np.array(tokens)\n",
    "print(\"Tokenize Done!\")\n",
    "\n",
    "train_X = tokens[:,0]\n",
    "train_Y = tokens[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y_ = W2V.One_hot(train_Y)  ## Convert to One-hot\n",
    "train_X_ = W2V.Convert2Vec(MODEL_DATA,train_X)  ## import word2vec model where you have trained before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- num_class : 긍정 or 부정 판별\n",
    "- keep_prob : drop out rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size = 32\n",
    "Total_size = len(train_X)\n",
    "Vector_size = 300\n",
    "seq_length = [len(x) for x in train_X]\n",
    "Maxseq_length = max(seq_length) ## 95\n",
    "learning_rate = 0.001\n",
    "lstm_units = 128\n",
    "num_class = 2\n",
    "training_epochs = 10\n",
    "keep_prob = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, Maxseq_length, Vector_size], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, shape = [None, num_class], name = 'Y')\n",
    "seq_len = tf.placeholder(tf.int32, shape = [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BiLSTM = Bi_LSTM.Bi_LSTM(lstm_units, num_class, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"loss\", reuse = tf.AUTO_REUSE):\n",
    "    logits = BiLSTM.logits(X, BiLSTM.W, BiLSTM.b, seq_len)\n",
    "    loss, optimizer = BiLSTM.model_build(logits, Y, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(logits)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = int(Total_size / Batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training!\")\n",
    "modelName = DATA_PATH + '/BiLSTM_model.ckpt'\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "\n",
    "    start_time = time.time()\n",
    "    sess.run(init)\n",
    "    train_writer = tf.summary.FileWriter(DATA_PATH, sess.graph)\n",
    "    merged = BiLSTM.graph_build()\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        avg_acc, avg_loss = 0. , 0.\n",
    "        for step in range(total_batch):\n",
    "\n",
    "            train_batch_X = train_X_[step*Batch_size : step*Batch_size+Batch_size]\n",
    "            train_batch_Y = train_Y_[step*Batch_size : step*Batch_size+Batch_size]\n",
    "            batch_seq_length = seq_length[step*Batch_size : step*Batch_size+Batch_size]\n",
    "            \n",
    "            train_batch_X = W2V.Zero_padding(train_batch_X, Batch_size, Maxseq_length, Vector_size)\n",
    "            \n",
    "            sess.run(optimizer, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            # Compute average loss\n",
    "            loss_ = sess.run(loss, feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            avg_loss += loss_ / total_batch\n",
    "            \n",
    "            acc = sess.run(accuracy , feed_dict={X: train_batch_X, Y: train_batch_Y, seq_len: batch_seq_length})\n",
    "            avg_acc += acc / total_batch\n",
    "            print(\"epoch : {:02d} step : {:04d} loss = {:.6f} accuracy= {:.6f}\".format(epoch+1, step+1, loss_, acc))\n",
    "   \n",
    "        summary = sess.run(merged, feed_dict = {BiLSTM.loss : avg_loss, BiLSTM.acc : avg_acc})       \n",
    "        train_writer.add_summary(summary, epoch)\n",
    "        \n",
    "    train_writer.close()\n",
    "    duration = time.time() - start_time\n",
    "    minute = int(duration / 60)\n",
    "    second = int(duration) % 60\n",
    "    print(\"%dminutes %dseconds\" % (minute,second))\n",
    "    save_path = saver.save(sess, modelName)\n",
    "    \n",
    "    print ('save_path',save_path)\n",
    "    \n",
    "    ## cmd 실행 -> cd C:\\Users\\jbk48\\Desktop\\Sentimental-Analysis-master\\Sentimental-Analysis-master\\Bidirectional_LSTM\n",
    "    ## tensorboard --logdir=./ 입력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
